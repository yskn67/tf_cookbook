{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_wrod_freq = 5\n",
    "rnn_size = 128\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_seq_len = 50\n",
    "embedding_size = rnn_size\n",
    "save_every = 500\n",
    "eval_every = 50\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "    \n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "print('Loading Shakespeare Data')\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found. downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    s_text = s_text[7675:]\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    word_counts = {key: val for key, val in word_counts.items() if val > min_word_freq}\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key: (ix + 1) for ix, key in enumerate(words)}\n",
    "    vocab_to_ix_dict['unknown'] = 0\n",
    "    ix_to_vocab_dict = {val: key for key, val in vocab_to_ix_dict.items()}\n",
    "    return (ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_wrod_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    \n",
    "    def __init__(self, rnn_size, batch_size, learning_rate,\n",
    "                training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_target = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.rnn_size], tf.float32, tf.random_normal_initializer())\n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "            \n",
    "        def inferred_loop(prev, count):\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return output\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(\n",
    "            rnn_inputs_trimmed,\n",
    "            self.initial_state,\n",
    "            self.lstm_cell,\n",
    "            loop_function=inferred_loop if infer_sample else None)\n",
    "        \n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun(\n",
    "            [self.logit_output],\n",
    "            [tf.reshape(self.y_target, [-1])],\n",
    "            [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "            self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10,\n",
    "               prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "        \n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [model_output, state] = sess.run(\n",
    "                [self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return out_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-cf8cd22a347d>:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                       training_seq_len, vocab_size)\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "num_batches = int(len(s_text_ix) / (batch_size * training_seq_len)) + 1\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.82\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 8.93\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.60\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.39\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 7.88\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art kate strew'd fantasy innocent to the\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.69\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.48\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.44\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.23\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.15\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art but for your\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.21\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 7.07\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.66\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.91\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.85\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.54\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.78\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.26\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.55\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.63\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.54\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.21\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.35\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.40\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.24\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.24\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.43\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.46\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.25\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.27\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art thou art thou art thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.38\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.37\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.29\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.26\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.16\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.40\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.16\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.26\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.12\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.21\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.22\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.36\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 5.96\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.08\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.13\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art a\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.45\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.45\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.49\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.49\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.42\n",
      "Model Saved To: ../data/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art car whisper thy\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.30\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.25\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.32\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.06\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 6.17\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art might have been\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 5.89\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.14\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.19\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 5.83\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.00\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.05\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.24\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.19\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.29\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.04\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 5.74\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.12\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 6.15\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.13\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 6.19\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.28\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.05\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 6.07\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 5.82\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 5.81\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 6.14\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 5.83\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 5.91\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 6.22\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 6.04\n",
      "thou art more\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 6.09\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.16\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.09\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 5.94\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 5.90\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 6.09\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 6.09\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.01\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.13\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.78\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.86\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.92\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.96\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.77\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 6.02\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 6.01\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.95\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.70\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 5.97\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.94\n",
      "Model Saved To: ../data/shakespeare_model/model\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.07\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.91\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 5.95\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 6.11\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 6.12\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.94\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.67\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 5.88\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.87\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.91\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 6.07\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.89\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.96\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 6.06\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.84\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not for a\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.70\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.06\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.90\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.66\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.80\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.89\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 5.79\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.80\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.94\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 5.89\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.75\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.57\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.84\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.76\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.58\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.80\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.86\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.64\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.77\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.80\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 6.02\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.93\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.95\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.92\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.85\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.59\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.83\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.99\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.83\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.86\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou art not a\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.77\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.48\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.71\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.79\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.72\n",
      "Model Saved To: ../data/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 5.92\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.82\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 5.73\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.62\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 6.03\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.70\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 6.09\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.70\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.72\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.82\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.87\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.80\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.62\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 6.02\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.68\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt find thy\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 5.57\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.91\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.75\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.79\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.70\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.64\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.64\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 6.01\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.48\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.89\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.67\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.70\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 5.69\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.76\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.81\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou shalt be\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.87\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(batches)\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch + 1, epochs))\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch,\n",
    "                         lstm_model.y_target: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run(\n",
    "            [lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "            feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch + 1, ix + 1, num_batches + 1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        if iteration_count % save_every == 0:\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step=iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "        \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7d47b8588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
