{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yskn67/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "embedding_size = 200\n",
    "vocabulary_size = 10000\n",
    "generations = 10000\n",
    "print_loss_every = 1000\n",
    "\n",
    "num_sampled = int(batch_size / 2)\n",
    "window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "print_valid_every = 5000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name = '../data'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        tar = tarfile.open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path=save_folder_name)\n",
    "        tar.close()\n",
    "    \n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii', errors='ignore').decode())\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "    \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii', errors='ignore').decode())\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1] * len(pos_data) + [0] * len(neg_data)\n",
    "    \n",
    "    return (texts, target)\n",
    "\n",
    "texts, target = load_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(texts, stops):\n",
    "    texts = [x.lower() for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "texts = normalize_text(texts, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count = [['RARE', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    data = []\n",
    "    for sentences in sentences:\n",
    "        sentence_data = []\n",
    "        for word in word_dict:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        rand_sentences = sentences[np.random.randint(len(sentences))]\n",
    "        window_sequences = [rand_sentences[max((ix - window_size), 0): (ix + window_size + 1)]\n",
    "                               for ix, x in enumerate(rand_sentences)]\n",
    "        label_indices = [ix if ix < window_size else window_size\n",
    "                            for ix, x in enumerate(window_sequences)]\n",
    "        \n",
    "        if method == 'skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y + 1):])\n",
    "                                   for x, y in zip(window_sequences, label_indices)]\n",
    "            tuple_data = [(x, y_) for x, y in batch_and_labels for y_ in y]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "        \n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "        \n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000 : 35.97349548339844\n",
      "Loss at step 2000 : 3.574927568435669\n",
      "Loss at step 3000 : 3.5766685009002686\n",
      "Loss at step 4000 : 2.932534694671631\n",
      "Loss at step 5000 : 2.750774621963501\n",
      "Nearest to cliche: impresses, distinct, frequent, vampires, savour,\n",
      "Nearest to love: lowtech, appetite, indicates, heartbreaking, flatter,\n",
      "Nearest to hate: sweetheart, sustained, smug, leap, odyssey,\n",
      "Nearest to silly: illuminates, connection, gripping, mouse, soso,\n",
      "Nearest to sad: elicit, undertone, ways, regular, sketchiest,\n",
      "Loss at step 6000 : 3.090549945831299\n",
      "Loss at step 7000 : 2.972794532775879\n",
      "Loss at step 8000 : 4.028219223022461\n",
      "Loss at step 9000 : 3.059959650039673\n",
      "Loss at step 10000 : 2.7488033771514893\n",
      "Nearest to cliche: impresses, distinct, frequent, vampires, savour,\n",
      "Nearest to love: lowtech, appetite, indicates, heartbreaking, flatter,\n",
      "Nearest to hate: sweetheart, sustained, smug, leap, odyssey,\n",
      "Nearest to silly: illuminates, connection, gripping, mouse, soso,\n",
      "Nearest to sad: elicit, undertone, ways, regular, sketchiest,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, x_inputs)\n",
    "    \n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                 stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=y_target,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=num_sampled,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    loss_vec = []\n",
    "    loss_x_vec = []\n",
    "    for i in range(generations):\n",
    "        batch_input, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "        feed_dict = {x_inputs: batch_input, y_target: batch_labels}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if (i + 1) % print_loss_every == 0:\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            loss_vec.append(loss_val)\n",
    "            loss_x_vec.append(i + 1)\n",
    "            print(\"Loss at step {} : {}\".format(i + 1, loss_val))\n",
    "            \n",
    "        if (i + 1) % print_valid_every == 0:\n",
    "            sim = sess.run(similarity)\n",
    "            for j in range(len(valid_words)):\n",
    "                valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "                top_k = 5\n",
    "                nearest = (-sim[j, :]).argsort()[1: top_k + 1]\n",
    "                log_str = \"Nearest to {}:\".format(valid_word)\n",
    "                for k in range(top_k):\n",
    "                    close_word = word_dictionary_rev[nearest[k]]\n",
    "                    log_str = \"{} {},\".format(log_str, close_word)\n",
    "                print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
