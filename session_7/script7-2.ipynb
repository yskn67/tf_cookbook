{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = os.path.join(\"..\", \"data\", \"temp_spam_data.csv\")\n",
    "\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read(\"SMSSpamCollection\")\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode(\"ascii\", errors=\"ignore\")\n",
    "    text_data = text_data.decode().split(\"\\n\")\n",
    "    text_data = [x.split(\"\\t\") for x in text_data if len(x) >= 1]\n",
    "    \n",
    "    with open(save_file_name, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(text_data)\n",
    "        \n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]\n",
    "target = [1 if x == \"spam\" else 0 for x in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [x.lower() for x in texts]\n",
    "texts = [\"\".join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "texts = [\"\".join(c for c in x if c not in \"0123456789\") for x in texts]\n",
    "texts = [\" \".join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5018c61208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title(\"Histogram of # of Words in Texts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = 25\n",
    "min_word_freq = 3\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(\n",
    "    sentence_size, min_frequency=min_word_freq)\n",
    "vocab_processor.fit_transform(texts)\n",
    "embedding_size = len(vocab_processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(texts), round(len(texts) * 0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #10: Loss = 17.34446\n",
      "Training Observation #20: Loss = 0.003193958\n",
      "Training Observation #30: Loss = 8.025453\n",
      "Training Observation #40: Loss = 6.6635866\n",
      "Training Observation #50: Loss = 5.7880077\n",
      "Training Observation #60: Loss = 5.513802\n",
      "Training Observation #70: Loss = 0.09874808\n",
      "Training Observation #80: Loss = 6.58816\n",
      "Training Observation #90: Loss = 0.13252527\n",
      "Training Observation #100: Loss = 0.6846022\n",
      "Training Observation #110: Loss = 0.030152822\n",
      "Training Observation #120: Loss = 3.5338133e-05\n",
      "Training Observation #130: Loss = 0.013697386\n",
      "Training Observation #140: Loss = 0.06408716\n",
      "Training Observation #150: Loss = 0.003058608\n",
      "Training Observation #160: Loss = 0.16892466\n",
      "Training Observation #170: Loss = 0.4893214\n",
      "Training Observation #180: Loss = 0.0040669776\n",
      "Training Observation #190: Loss = 3.269542\n",
      "Training Observation #200: Loss = 1.0105908\n",
      "Training Observation #210: Loss = 0.009572016\n",
      "Training Observation #220: Loss = 5.3245807\n",
      "Training Observation #230: Loss = 0.5239229\n",
      "Training Observation #240: Loss = 5.4963193\n",
      "Training Observation #250: Loss = 1.3472638\n",
      "Training Observation #260: Loss = 0.003337067\n",
      "Training Observation #270: Loss = 0.26005822\n",
      "Training Observation #280: Loss = 1.1705064\n",
      "Training Observation #290: Loss = 0.13848805\n",
      "Training Observation #300: Loss = 2.4622035e-05\n",
      "Training Observation #310: Loss = 0.00030923748\n",
      "Training Observation #320: Loss = 0.2984724\n",
      "Training Observation #330: Loss = 0.32032242\n",
      "Training Observation #340: Loss = 0.0010434373\n",
      "Training Observation #350: Loss = 0.00033598425\n",
      "Training Observation #360: Loss = 0.34834296\n",
      "Training Observation #370: Loss = 0.00059027603\n",
      "Training Observation #380: Loss = 1.7389706\n",
      "Training Observation #390: Loss = 7.4487198e-06\n",
      "Training Observation #400: Loss = 0.0013988372\n",
      "Training Observation #410: Loss = 1.8492272\n",
      "Training Observation #420: Loss = 0.05286305\n",
      "Training Observation #430: Loss = 2.0897225e-05\n",
      "Training Observation #440: Loss = 2.2526731\n",
      "Training Observation #450: Loss = 0.014093617\n",
      "Training Observation #460: Loss = 6.3789425\n",
      "Training Observation #470: Loss = 6.0585003\n",
      "Training Observation #480: Loss = 0.000651536\n",
      "Training Observation #490: Loss = 3.2224393\n",
      "Training Observation #500: Loss = 0.004530706\n",
      "Training Observation #510: Loss = 0.0019790893\n",
      "Training Observation #520: Loss = 1.3505457e-05\n",
      "Training Observation #530: Loss = 0.012429503\n",
      "Training Observation #540: Loss = 0.00030327513\n",
      "Training Observation #550: Loss = 0.00017201976\n",
      "Training Observation #560: Loss = 1.126157\n",
      "Training Observation #570: Loss = 0.00030386768\n",
      "Training Observation #580: Loss = 2.883108\n",
      "Training Observation #590: Loss = 0.0010471477\n",
      "Training Observation #600: Loss = 0.00017491798\n",
      "Training Observation #610: Loss = 0.00011319695\n",
      "Training Observation #620: Loss = 0.009210567\n",
      "Training Observation #630: Loss = 0.0047283955\n",
      "Training Observation #640: Loss = 1.0569478e-08\n",
      "Training Observation #650: Loss = 0.8231119\n",
      "Training Observation #660: Loss = 0.032343365\n",
      "Training Observation #670: Loss = 1.19736505e-05\n",
      "Training Observation #680: Loss = 0.00019852674\n",
      "Training Observation #690: Loss = 9.4319e-05\n",
      "Training Observation #700: Loss = 0.009202819\n",
      "Training Observation #710: Loss = 0.03652411\n",
      "Training Observation #720: Loss = 0.0004955278\n",
      "Training Observation #730: Loss = 1.0738293\n",
      "Training Observation #740: Loss = 8.437111e-05\n",
      "Training Observation #750: Loss = 9.329201e-06\n",
      "Training Observation #760: Loss = 2.2507718e-05\n",
      "Training Observation #770: Loss = 4.155076\n",
      "Training Observation #780: Loss = 0.0020812077\n",
      "Training Observation #790: Loss = 0.006212615\n",
      "Training Observation #800: Loss = 0.0015481754\n",
      "Training Observation #810: Loss = 5.001419\n",
      "Training Observation #820: Loss = 4.7650805\n",
      "Training Observation #830: Loss = 0.3135488\n",
      "Training Observation #840: Loss = 1.6334103e-05\n",
      "Training Observation #850: Loss = 0.0008496425\n",
      "Training Observation #860: Loss = 2.63187e-06\n",
      "Training Observation #870: Loss = 0.0014816157\n",
      "Training Observation #880: Loss = 2.5679638\n",
      "Training Observation #890: Loss = 0.00033381584\n",
      "Training Observation #900: Loss = 3.3281717e-07\n",
      "Training Observation #910: Loss = 0.0048584254\n",
      "Training Observation #920: Loss = 3.107467\n",
      "Training Observation #930: Loss = 0.0003583237\n",
      "Training Observation #940: Loss = 5.281789\n",
      "Training Observation #950: Loss = 0.001834686\n",
      "Training Observation #960: Loss = 0.6216519\n",
      "Training Observation #970: Loss = 7.5824944e-05\n",
      "Training Observation #980: Loss = 0.001791964\n",
      "Training Observation #990: Loss = 1.0024463e-05\n",
      "Training Observation #1000: Loss = 3.6754386\n",
      "Training Observation #1010: Loss = 0.01220399\n",
      "Training Observation #1020: Loss = 1.1252918\n",
      "Training Observation #1030: Loss = 0.012092745\n",
      "Training Observation #1040: Loss = 7.0630245e-05\n",
      "Training Observation #1050: Loss = 0.0064696237\n",
      "Training Observation #1060: Loss = 3.7867285e-05\n",
      "Training Observation #1070: Loss = 0.0034127885\n",
      "Training Observation #1080: Loss = 6.1471095e-05\n",
      "Training Observation #1090: Loss = 5.8812988e-05\n",
      "Training Observation #1100: Loss = 0.025472142\n",
      "Training Observation #1110: Loss = 0.00079752214\n",
      "Training Observation #1120: Loss = 3.042118e-05\n",
      "Training Observation #1130: Loss = 0.0022019832\n",
      "Training Observation #1140: Loss = 0.08065269\n",
      "Training Observation #1150: Loss = 0.00024737784\n",
      "Training Observation #1160: Loss = 0.00933624\n",
      "Training Observation #1170: Loss = 0.00017153259\n",
      "Training Observation #1180: Loss = 0.0059574107\n",
      "Training Observation #1190: Loss = 0.2152401\n",
      "Training Observation #1200: Loss = 0.24101996\n",
      "Training Observation #1210: Loss = 2.5373865e-06\n",
      "Training Observation #1220: Loss = 0.32033035\n",
      "Training Observation #1230: Loss = 0.96897495\n",
      "Training Observation #1240: Loss = 4.2817955\n",
      "Training Observation #1250: Loss = 0.043578483\n",
      "Training Observation #1260: Loss = 1.2492382e-05\n",
      "Training Observation #1270: Loss = 0.02687276\n",
      "Training Observation #1280: Loss = 0.0029523885\n",
      "Training Observation #1290: Loss = 1.7295893\n",
      "Training Observation #1300: Loss = 8.771059e-05\n",
      "Training Observation #1310: Loss = 5.431617\n",
      "Training Observation #1320: Loss = 0.21713778\n",
      "Training Observation #1330: Loss = 0.018651139\n",
      "Training Observation #1340: Loss = 0.0008539655\n",
      "Training Observation #1350: Loss = 0.0009575169\n",
      "Training Observation #1360: Loss = 0.71565056\n",
      "Training Observation #1370: Loss = 0.19700961\n",
      "Training Observation #1380: Loss = 0.2174933\n",
      "Training Observation #1390: Loss = 4.9597667e-05\n",
      "Training Observation #1400: Loss = 0.20116551\n",
      "Training Observation #1410: Loss = 0.0011454906\n",
      "Training Observation #1420: Loss = 8.488268e-05\n",
      "Training Observation #1430: Loss = 0.08777266\n",
      "Training Observation #1440: Loss = 0.0007201441\n",
      "Training Observation #1450: Loss = 3.0360607e-05\n",
      "Training Observation #1460: Loss = 0.101067506\n",
      "Training Observation #1470: Loss = 0.11816634\n",
      "Training Observation #1480: Loss = 1.8350255e-06\n",
      "Training Observation #1490: Loss = 2.5866459e-05\n",
      "Training Observation #1500: Loss = 0.01007699\n",
      "Training Observation #1510: Loss = 0.0062599364\n",
      "Training Observation #1520: Loss = 0.07299728\n",
      "Training Observation #1530: Loss = 2.1566257\n",
      "Training Observation #1540: Loss = 0.011629029\n",
      "Training Observation #1550: Loss = 0.00032691023\n",
      "Training Observation #1560: Loss = 0.00040913787\n",
      "Training Observation #1570: Loss = 0.00013069158\n",
      "Training Observation #1580: Loss = 0.0005261184\n",
      "Training Observation #1590: Loss = 0.27558786\n",
      "Training Observation #1600: Loss = 0.00070957217\n",
      "Training Observation #1610: Loss = 4.750314e-05\n",
      "Training Observation #1620: Loss = 0.056831736\n",
      "Training Observation #1630: Loss = 0.7154507\n",
      "Training Observation #1640: Loss = 1.7281971\n",
      "Training Observation #1650: Loss = 9.95423e-05\n",
      "Training Observation #1660: Loss = 1.0478692\n",
      "Training Observation #1670: Loss = 9.811648\n",
      "Training Observation #1680: Loss = 0.009954679\n",
      "Training Observation #1690: Loss = 1.2214386e-05\n",
      "Training Observation #1700: Loss = 0.0072555332\n",
      "Training Observation #1710: Loss = 0.022877838\n",
      "Training Observation #1720: Loss = 0.008808541\n",
      "Training Observation #1730: Loss = 0.0068525327\n",
      "Training Observation #1740: Loss = 0.0037280514\n",
      "Training Observation #1750: Loss = 0.00028519108\n",
      "Training Observation #1760: Loss = 0.00045195362\n",
      "Training Observation #1770: Loss = 5.5695877\n",
      "Training Observation #1780: Loss = 0.0007776457\n",
      "Training Observation #1790: Loss = 0.00045414452\n",
      "Training Observation #1800: Loss = 0.0037070669\n",
      "Training Observation #1810: Loss = 0.0015473238\n",
      "Training Observation #1820: Loss = 0.04526423\n",
      "Training Observation #1830: Loss = 0.005633804\n",
      "Training Observation #1840: Loss = 0.00037187347\n",
      "Training Observation #1850: Loss = 0.00015951593\n",
      "Training Observation #1860: Loss = 0.001295426\n",
      "Training Observation #1870: Loss = 8.390428\n",
      "Training Observation #1880: Loss = 2.653997\n",
      "Training Observation #1890: Loss = 0.005076056\n",
      "Training Observation #1900: Loss = 0.47804558\n",
      "Training Observation #1910: Loss = 3.5429985e-05\n",
      "Training Observation #1920: Loss = 2.2173039e-05\n",
      "Training Observation #1930: Loss = 0.0001028066\n",
      "Training Observation #1940: Loss = 4.4200282e-05\n",
      "Training Observation #1950: Loss = 9.7946424e-05\n",
      "Training Observation #1960: Loss = 9.430757e-07\n",
      "Training Observation #1970: Loss = 1.2353604e-05\n",
      "Training Observation #1980: Loss = 0.033315275\n",
      "Training Observation #1990: Loss = 0.2562419\n",
      "Training Observation #2000: Loss = 3.544827e-05\n",
      "Training Observation #2010: Loss = 0.68533665\n",
      "Training Observation #2020: Loss = 8.752202e-05\n",
      "Training Observation #2030: Loss = 0.00014481845\n",
      "Training Observation #2040: Loss = 3.1659956\n",
      "Training Observation #2050: Loss = 1.6970898e-05\n",
      "Training Observation #2060: Loss = 6.4817724\n",
      "Training Observation #2070: Loss = 7.626917e-06\n",
      "Training Observation #2080: Loss = 1.4002862e-06\n",
      "Training Observation #2090: Loss = 2.2658925e-05\n",
      "Training Observation #2100: Loss = 3.3267497e-05\n",
      "Training Observation #2110: Loss = 0.00023081065\n",
      "Training Observation #2120: Loss = 11.026684\n",
      "Training Observation #2130: Loss = 0.40162843\n",
      "Training Observation #2140: Loss = 1.8590163\n",
      "Training Observation #2150: Loss = 0.07109094\n",
      "Training Observation #2160: Loss = 6.1640574e-05\n",
      "Training Observation #2170: Loss = 0.0035141653\n",
      "Training Observation #2180: Loss = 0.0009099932\n",
      "Training Observation #2190: Loss = 0.00012916724\n",
      "Training Observation #2200: Loss = 0.0035539367\n",
      "Training Observation #2210: Loss = 2.8438908e-05\n",
      "Training Observation #2220: Loss = 0.0010215086\n",
      "Training Observation #2230: Loss = 2.5450849e-05\n",
      "Training Observation #2240: Loss = 0.00018908034\n",
      "Training Observation #2250: Loss = 3.754824\n",
      "Training Observation #2260: Loss = 0.0008122313\n",
      "Training Observation #2270: Loss = 1.1706431\n",
      "Training Observation #2280: Loss = 2.0021176\n",
      "Training Observation #2290: Loss = 4.9845232e-05\n",
      "Training Observation #2300: Loss = 0.004540574\n",
      "Training Observation #2310: Loss = 9.878758e-06\n",
      "Training Observation #2320: Loss = 0.86088675\n",
      "Training Observation #2330: Loss = 0.004406437\n",
      "Training Observation #2340: Loss = 1.3219704e-07\n",
      "Training Observation #2350: Loss = 0.017887525\n",
      "Training Observation #2360: Loss = 1.695068e-05\n",
      "Training Observation #2370: Loss = 4.2264814\n",
      "Training Observation #2380: Loss = 0.00020450832\n",
      "Training Observation #2390: Loss = 4.3371398e-05\n",
      "Training Observation #2400: Loss = 0.0026821042\n",
      "Training Observation #2410: Loss = 0.10146697\n",
      "Training Observation #2420: Loss = 0.0064479136\n",
      "Training Observation #2430: Loss = 0.014385337\n",
      "Training Observation #2440: Loss = 0.027624076\n",
      "Training Observation #2450: Loss = 3.0216757e-05\n",
      "Training Observation #2460: Loss = 1.7356322e-05\n",
      "Training Observation #2470: Loss = 4.919266\n",
      "Training Observation #2480: Loss = 5.0303173\n",
      "Training Observation #2490: Loss = 0.12458794\n",
      "Training Observation #2500: Loss = 0.053880557\n",
      "Training Observation #2510: Loss = 7.7770452\n",
      "Training Observation #2520: Loss = 0.00012668599\n",
      "Training Observation #2530: Loss = 0.042601563\n",
      "Training Observation #2540: Loss = 0.0056258077\n",
      "Training Observation #2550: Loss = 2.8617897e-06\n",
      "Training Observation #2560: Loss = 0.03031679\n",
      "Training Observation #2570: Loss = 0.07186347\n",
      "Training Observation #2580: Loss = 2.7712135\n",
      "Training Observation #2590: Loss = 1.4100941e-05\n",
      "Training Observation #2600: Loss = 0.2736328\n",
      "Training Observation #2610: Loss = 2.5595884e-06\n",
      "Training Observation #2620: Loss = 1.7076938\n",
      "Training Observation #2630: Loss = 1.0073106\n",
      "Training Observation #2640: Loss = 5.514885e-06\n",
      "Training Observation #2650: Loss = 0.0035358367\n",
      "Training Observation #2660: Loss = 0.03307438\n",
      "Training Observation #2670: Loss = 0.0102389855\n",
      "Training Observation #2680: Loss = 0.0026005958\n",
      "Training Observation #2690: Loss = 5.5556693e-06\n",
      "Training Observation #2700: Loss = 7.8762393\n",
      "Training Observation #2710: Loss = 0.010886296\n",
      "Training Observation #2720: Loss = 0.042127926\n",
      "Training Observation #2730: Loss = 0.015582526\n",
      "Training Observation #2740: Loss = 4.962495\n",
      "Training Observation #2750: Loss = 1.4240396\n",
      "Training Observation #2760: Loss = 0.32365933\n",
      "Training Observation #2770: Loss = 0.0005699505\n",
      "Training Observation #2780: Loss = 0.11502466\n",
      "Training Observation #2790: Loss = 0.053784084\n",
      "Training Observation #2800: Loss = 0.06530261\n",
      "Training Observation #2810: Loss = 0.0017509342\n",
      "Training Observation #2820: Loss = 0.0003631352\n",
      "Training Observation #2830: Loss = 0.53484505\n",
      "Training Observation #2840: Loss = 0.009303997\n",
      "Training Observation #2850: Loss = 0.0022483622\n",
      "Training Observation #2860: Loss = 0.057315238\n",
      "Training Observation #2870: Loss = 0.00025552994\n",
      "Training Observation #2880: Loss = 0.00011971173\n",
      "Training Observation #2890: Loss = 0.004464593\n",
      "Training Observation #2900: Loss = 0.9190032\n",
      "Training Observation #2910: Loss = 7.273106e-05\n",
      "Training Observation #2920: Loss = 0.0002444355\n",
      "Training Observation #2930: Loss = 0.00022359457\n",
      "Training Observation #2940: Loss = 0.0015016038\n",
      "Training Observation #2950: Loss = 0.00023657436\n",
      "Training Observation #2960: Loss = 0.0067110895\n",
      "Training Observation #2970: Loss = 5.2101568e-05\n",
      "Training Observation #2980: Loss = 0.0017642573\n",
      "Training Observation #2990: Loss = 5.774398\n",
      "Training Observation #3000: Loss = 0.00089130097\n",
      "Training Observation #3010: Loss = 0.005317535\n",
      "Training Observation #3020: Loss = 0.97368056\n",
      "Training Observation #3030: Loss = 0.0071823336\n",
      "Training Observation #3040: Loss = 6.031724\n",
      "Training Observation #3050: Loss = 0.0005445091\n",
      "Training Observation #3060: Loss = 1.8039507e-05\n",
      "Training Observation #3070: Loss = 3.3319575e-06\n",
      "Training Observation #3080: Loss = 2.9800829e-05\n",
      "Training Observation #3090: Loss = 0.00064350653\n",
      "Training Observation #3100: Loss = 0.008711363\n",
      "Training Observation #3110: Loss = 0.0005732123\n",
      "Training Observation #3120: Loss = 0.48857522\n",
      "Training Observation #3130: Loss = 1.0389912e-06\n",
      "Training Observation #3140: Loss = 1.175192\n",
      "Training Observation #3150: Loss = 0.0009489457\n",
      "Training Observation #3160: Loss = 0.00018290903\n",
      "Training Observation #3170: Loss = 0.4383764\n",
      "Training Observation #3180: Loss = 0.05561101\n",
      "Training Observation #3190: Loss = 4.110732\n",
      "Training Observation #3200: Loss = 6.8406666e-06\n",
      "Training Observation #3210: Loss = 0.00012615444\n",
      "Training Observation #3220: Loss = 0.046775825\n",
      "Training Observation #3230: Loss = 2.1733534\n",
      "Training Observation #3240: Loss = 4.586941\n",
      "Training Observation #3250: Loss = 0.30599964\n",
      "Training Observation #3260: Loss = 8.906493e-05\n",
      "Training Observation #3270: Loss = 6.4537113e-07\n",
      "Training Observation #3280: Loss = 1.0863495e-06\n",
      "Training Observation #3290: Loss = 0.00014647776\n",
      "Training Observation #3300: Loss = 0.012154814\n",
      "Training Observation #3310: Loss = 0.0015418829\n",
      "Training Observation #3320: Loss = 0.1621055\n",
      "Training Observation #3330: Loss = 0.00021239475\n",
      "Training Observation #3340: Loss = 0.00015116588\n",
      "Training Observation #3350: Loss = 0.02821848\n",
      "Training Observation #3360: Loss = 3.6027097e-05\n",
      "Training Observation #3370: Loss = 0.0012741325\n",
      "Training Observation #3380: Loss = 4.2463703\n",
      "Training Observation #3390: Loss = 2.766275e-05\n",
      "Training Observation #3400: Loss = 0.00065012\n",
      "Training Observation #3410: Loss = 0.0006810031\n",
      "Training Observation #3420: Loss = 0.0031947505\n",
      "Training Observation #3430: Loss = 0.99878556\n",
      "Training Observation #3440: Loss = 0.071191214\n",
      "Training Observation #3450: Loss = 0.45500553\n",
      "Training Observation #3460: Loss = 0.0003128633\n",
      "Training Observation #3470: Loss = 7.515982\n",
      "Training Observation #3480: Loss = 0.0021987497\n",
      "Training Observation #3490: Loss = 3.5030363\n",
      "Training Observation #3500: Loss = 0.5369184\n",
      "Training Observation #3510: Loss = 2.5646565\n",
      "Training Observation #3520: Loss = 2.0069165\n",
      "Training Observation #3530: Loss = 0.000484344\n",
      "Training Observation #3540: Loss = 0.026810369\n",
      "Training Observation #3550: Loss = 0.0006093018\n",
      "Training Observation #3560: Loss = 7.101697\n",
      "Training Observation #3570: Loss = 0.00014146492\n",
      "Training Observation #3580: Loss = 0.0029911974\n",
      "Training Observation #3590: Loss = 0.0002593691\n",
      "Training Observation #3600: Loss = 0.0003295024\n",
      "Training Observation #3610: Loss = 0.0014754675\n",
      "Training Observation #3620: Loss = 0.00045082366\n",
      "Training Observation #3630: Loss = 9.108565\n",
      "Training Observation #3640: Loss = 5.640849e-05\n",
      "Training Observation #3650: Loss = 0.0010099564\n",
      "Training Observation #3660: Loss = 1.9951733\n",
      "Training Observation #3670: Loss = 6.154959\n",
      "Training Observation #3680: Loss = 7.448602e-05\n",
      "Training Observation #3690: Loss = 0.00014342897\n",
      "Training Observation #3700: Loss = 1.1228178e-05\n",
      "Training Observation #3710: Loss = 5.166625e-05\n",
      "Training Observation #3720: Loss = 1.5752995\n",
      "Training Observation #3730: Loss = 0.0056311605\n",
      "Training Observation #3740: Loss = 0.20344485\n",
      "Training Observation #3750: Loss = 0.016747696\n",
      "Training Observation #3760: Loss = 0.0313828\n",
      "Training Observation #3770: Loss = 3.3093398e-05\n",
      "Training Observation #3780: Loss = 0.0007011085\n",
      "Training Observation #3790: Loss = 0.0011574064\n",
      "Training Observation #3800: Loss = 0.69938016\n",
      "Training Observation #3810: Loss = 0.0004895377\n",
      "Training Observation #3820: Loss = 0.00064264116\n",
      "Training Observation #3830: Loss = 0.002293067\n",
      "Training Observation #3840: Loss = 0.4728144\n",
      "Training Observation #3850: Loss = 0.00030754664\n",
      "Training Observation #3860: Loss = 0.009153035\n",
      "Training Observation #3870: Loss = 4.7198553\n",
      "Training Observation #3880: Loss = 0.0025081115\n",
      "Training Observation #3890: Loss = 0.00023455231\n",
      "Training Observation #3900: Loss = 9.673247e-05\n",
      "Training Observation #3910: Loss = 0.0004908567\n",
      "Training Observation #3920: Loss = 4.820143e-05\n",
      "Training Observation #3930: Loss = 0.0106238155\n",
      "Training Observation #3940: Loss = 0.0020388442\n",
      "Training Observation #3950: Loss = 0.00025817656\n",
      "Training Observation #3960: Loss = 0.18501244\n",
      "Training Observation #3970: Loss = 0.111023255\n",
      "Training Observation #3980: Loss = 0.04535472\n",
      "Training Observation #3990: Loss = 0.00062600675\n",
      "Training Observation #4000: Loss = 0.005062508\n",
      "Training Observation #4010: Loss = 0.00038668307\n",
      "Training Observation #4020: Loss = 0.051334992\n",
      "Training Observation #4030: Loss = 0.0041035404\n",
      "Training Observation #4040: Loss = 2.573282\n",
      "Training Observation #4050: Loss = 0.00044230509\n",
      "Training Observation #4060: Loss = 0.001896382\n",
      "Training Observation #4070: Loss = 0.22045337\n",
      "Training Observation #4080: Loss = 0.08544977\n",
      "Training Observation #4090: Loss = 0.00011172024\n",
      "Training Observation #4100: Loss = 0.00047929786\n",
      "Training Observation #4110: Loss = 4.164846e-08\n",
      "Training Observation #4120: Loss = 0.0006909993\n",
      "Training Observation #4130: Loss = 0.09277719\n",
      "Training Observation #4140: Loss = 0.0068700355\n",
      "Training Observation #4150: Loss = 0.0001232426\n",
      "Training Observation #4160: Loss = 0.0004978564\n",
      "Training Observation #4170: Loss = 0.0011866872\n",
      "Training Observation #4180: Loss = 0.019567883\n",
      "Training Observation #4190: Loss = 1.3175694e-05\n",
      "Training Observation #4200: Loss = 0.0011730524\n",
      "Training Observation #4210: Loss = 1.47354785e-05\n",
      "Training Observation #4220: Loss = 0.002493472\n",
      "Training Observation #4230: Loss = 3.353343e-05\n",
      "Training Observation #4240: Loss = 0.000229507\n",
      "Training Observation #4250: Loss = 0.00011043558\n",
      "Training Observation #4260: Loss = 1.617097e-05\n",
      "Training Observation #4270: Loss = 2.9172454e-06\n",
      "Training Observation #4280: Loss = 0.00014997128\n",
      "Training Observation #4290: Loss = 0.00051350077\n",
      "Training Observation #4300: Loss = 0.77297723\n",
      "Training Observation #4310: Loss = 0.00026158834\n",
      "Training Observation #4320: Loss = 0.0013156842\n",
      "Training Observation #4330: Loss = 0.0017844187\n",
      "Training Observation #4340: Loss = 4.9196125e-05\n",
      "Training Observation #4350: Loss = 0.070367135\n",
      "Training Observation #4360: Loss = 4.3716725e-05\n",
      "Training Observation #4370: Loss = 0.09556196\n",
      "Training Observation #4380: Loss = 0.01616947\n",
      "Training Observation #4390: Loss = 6.6704597\n",
      "Training Observation #4400: Loss = 0.00015444183\n",
      "Training Observation #4410: Loss = 0.002634577\n",
      "Training Observation #4420: Loss = 8.280892e-05\n",
      "Training Observation #4430: Loss = 0.016555289\n",
      "Training Observation #4440: Loss = 0.00057580933\n",
      "Training Observation #4450: Loss = 0.00014057267\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "overall Test Accuracy: 0.8044843049327354\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
    "    A = tf.Variable(tf.random_normal(shape=[embedding_size, 1]))\n",
    "    b = tf.Variable(tf.random_normal(shape=[1, 1]))\n",
    "    x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "    y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "    x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "    x_col_sums = tf.reduce_sum(x_embed, 0)\n",
    "    x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "    model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=model_output,\n",
    "        labels=y_target\n",
    "    ))\n",
    "    prediction = tf.sigmoid(model_output)\n",
    "    \n",
    "    my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "    train_step = my_opt.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    loss_vec = []\n",
    "    train_acc_all = []\n",
    "    train_acc_avg = []\n",
    "    for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "        y_data = [[target_train[ix]]]\n",
    "        sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "        \n",
    "        temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "        loss_vec.append(temp_loss)\n",
    "        \n",
    "        if (ix + 1) % 10 == 0:\n",
    "            print('Training Observation #' + str(ix + 1) + ': Loss = ' + str(temp_loss))\n",
    "        \n",
    "        [[temp_pred]] = sess.run(prediction, feed_dict={x_data: t, y_target: y_data})\n",
    "        \n",
    "        train_acc_temp = target_train[ix] == np.round(temp_pred)\n",
    "        train_acc_all.append(train_acc_temp)\n",
    "        if len(train_acc_all) >= 50:\n",
    "            train_acc_avg.append(np.mean(train_acc_all[-50:]))\n",
    "    \n",
    "    test_acc_all = []\n",
    "    for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "        y_data = [[target_test[ix]]]\n",
    "        sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "        \n",
    "        if (ix + 1) % 50 == 0:\n",
    "            print('Test Observation #' + str(ix + 1))\n",
    "        \n",
    "        [[temp_pred]] = sess.run(prediction, feed_dict={x_data: t, y_target: y_data})\n",
    "        \n",
    "        test_acc_temp = target_test[ix] == np.round(temp_pred)\n",
    "        test_acc_all.append(test_acc_temp)\n",
    "\n",
    "    print('overall Test Accuracy: {}'.format(np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
