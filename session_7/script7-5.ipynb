{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yskn67/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "embedding_size = 200\n",
    "vocabulary_size = 2000\n",
    "generations = 10000\n",
    "model_learning_rate = 0.05\n",
    "\n",
    "num_sampled = int(batch_size / 2)\n",
    "window_size = 3\n",
    "\n",
    "save_embeddings_every = 5000\n",
    "print_valid_every = 5000\n",
    "print_loss_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name = '../data'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        tar = tarfile.open(os.path.join(save_folder_name, 'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path=save_folder_name)\n",
    "        tar.close()\n",
    "    \n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii', errors='ignore').decode())\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "    \n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii', errors='ignore').decode())\n",
    "    neg_data = [x.rstrip() for x in neg_data]\n",
    "    \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1] * len(pos_data) + [0] * len(neg_data)\n",
    "    \n",
    "    return (texts, target)\n",
    "\n",
    "texts, target = load_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(texts, stops):\n",
    "    texts = [x.lower() for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    return texts\n",
    "texts = normalize_text(texts, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    count = [['RARE', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    word_dict = {}\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    data = []\n",
    "    for sentences in sentences:\n",
    "        sentence_data = []\n",
    "        for word in word_dict:\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)\n",
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        rand_sentences = sentences[np.random.randint(len(sentences))]\n",
    "        window_sequences = [rand_sentences[max((ix - window_size), 0): (ix + window_size + 1)]\n",
    "                               for ix, x in enumerate(rand_sentences)]\n",
    "        label_indices = [ix if ix < window_size else window_size\n",
    "                            for ix, x in enumerate(window_sequences)]\n",
    "        \n",
    "        if method == 'skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y + 1):])\n",
    "                                   for x, y in zip(window_sequences, label_indices)]\n",
    "            tuple_data = [(x, y_) for x, y in batch_and_labels for y_ in y]\n",
    "            batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        elif method == 'cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y + 1):], x[y]) for x, y in zip(window_sequences, label_indices)]\n",
    "            batch_and_labels = [(x, y) for x, y in batch_and_labels if len(x) == 2 * window_size]\n",
    "            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "        \n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return (batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000 : 0.975059449672699\n",
      "Loss at step 2000 : 0.818419873714447\n",
      "Loss at step 3000 : 0.7739680409431458\n",
      "Loss at step 4000 : 0.7645130753517151\n",
      "Loss at step 5000 : 0.7452167272567749\n",
      "Nearest to love: fine, dialogue, things, others, probably,\n",
      "Nearest to hate: behind, sadly, ms, treat, energetic,\n",
      "Nearest to happy: questions, planet, example, throughout, project,\n",
      "Nearest to sad: problem, cliches, created, mindless, directing,\n",
      "Nearest to man: moving, project, inventive, dull, green,\n",
      "Nearest to woman: mere, degree, poor, believable, went,\n",
      "Model save in file: ./cbow_movie_embeddings.ckpt\n",
      "Loss at step 6000 : 0.7114600539207458\n",
      "Loss at step 7000 : 0.6900984048843384\n",
      "Loss at step 8000 : 0.7305145859718323\n",
      "Loss at step 9000 : 0.6865231394767761\n",
      "Loss at step 10000 : 0.7282634377479553\n",
      "Nearest to love: fine, dialogue, things, others, probably,\n",
      "Nearest to hate: behind, sadly, ms, treat, energetic,\n",
      "Nearest to happy: questions, planet, example, throughout, project,\n",
      "Nearest to sad: problem, cliches, created, mindless, directing,\n",
      "Nearest to man: moving, project, inventive, dull, green,\n",
      "Nearest to woman: mere, degree, poor, believable, went,\n",
      "Model save in file: ./cbow_movie_embeddings.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    x_inputs = tf.placeholder(tf.int32, shape=[batch_size, 2 * window_size])\n",
    "    y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    embed = tf.zeros([batch_size, embedding_size])\n",
    "    for element in range(2 * window_size):\n",
    "        embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n",
    "    \n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                 stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                         biases=nce_biases,\n",
    "                                         labels=y_target,\n",
    "                                         inputs=embed,\n",
    "                                         num_sampled=num_sampled,\n",
    "                                         num_classes=vocabulary_size))\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    saver = tf.train.Saver({\"embeddings\": embeddings})\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    text_data = [x for x in text_data if len(x) >= (2 * window_size + 1)]\n",
    "    \n",
    "    loss_vec = []\n",
    "    loss_x_vec = []\n",
    "    for i in range(generations):\n",
    "        batch_input, batch_labels = generate_batch_data(text_data, batch_size,\n",
    "                                                        window_size, method='cbow')\n",
    "        feed_dict = {x_inputs: batch_input, y_target: batch_labels}\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        \n",
    "        if (i + 1) % print_loss_every == 0:\n",
    "            loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "            loss_vec.append(loss_val)\n",
    "            loss_x_vec.append(i + 1)\n",
    "            print(\"Loss at step {} : {}\".format(i + 1, loss_val))\n",
    "            \n",
    "        if (i + 1) % print_valid_every == 0:\n",
    "            sim = sess.run(similarity)\n",
    "            for j in range(len(valid_words)):\n",
    "                valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "                top_k = 5\n",
    "                nearest = (-sim[j, :]).argsort()[1: top_k + 1]\n",
    "                log_str = \"Nearest to {}:\".format(valid_word)\n",
    "                for k in range(top_k):\n",
    "                    close_word = word_dictionary_rev[nearest[k]]\n",
    "                    log_str = \"{} {},\".format(log_str, close_word)\n",
    "                print(log_str)\n",
    "        \n",
    "        if (i + 1) % save_embeddings_every == 0:\n",
    "            with open('movie_vocab.pkl', 'wb') as f:\n",
    "                pickle.dump(word_dictionary, f)\n",
    "            model_checkpoint_path = './cbow_movie_embeddings.ckpt'\n",
    "            save_path = saver.save(sess, model_checkpoint_path)\n",
    "            print('Model save in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
